# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Models (These must be installed in Ollama)
EMBEDDING_MODEL=nomic-embed-text
LLM_MODEL=mistral

# Alternative models you can use:
# LLM_MODEL options: llama2, mistral, neural-chat, orca-mini, zephyr
# EMBEDDING_MODEL options: nomic-embed-text, all-minilm
